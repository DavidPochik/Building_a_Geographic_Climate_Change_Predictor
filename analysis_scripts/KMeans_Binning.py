import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.neighbors import KNeighborsClassifier
from sklearn.base import BaseEstimator, clone
from sklearn.inspection import DecisionBoundaryDisplay
from sklearn.utils.metaestimators import available_if
from sklearn.utils.validation import check_is_fitted

# load combined data file generated by gen_combined_csv.py
gsoy_df = pd.read_csv('data/combined_us48_GSOY_data.csv')

# train the clusterer on 1949 data, which we won't use in the eventual regressions
spatial_cluster_train = gsoy_df[gsoy_df['DATE']==1949]

# define an inductive clusterer where you can choose what clustering and classification algorithm you use 
# I got this from a sklearn tutorial
def _classifier_has(attr):
    """Check if we can delegate a method to the underlying classifier.

    First, we check the first fitted classifier if available, otherwise we
    check the unfitted classifier.
    """
    return lambda estimator: (
        hasattr(estimator.classifier_, attr)
        if hasattr(estimator, "classifier_")
        else hasattr(estimator.classifier, attr)
    )

class InductiveClusterer(BaseEstimator):
    def __init__(self, clusterer, classifier):
        self.clusterer = clusterer
        self.classifier = classifier

    def fit(self, X, y=None):
        self.clusterer_ = clone(self.clusterer)
        self.classifier_ = clone(self.classifier)
        y = self.clusterer_.fit_predict(X)
        self.classifier_.fit(X, y)
        return self

    @available_if(_classifier_has("predict"))
    def predict(self, X):
        check_is_fitted(self)
        return self.classifier_.predict(X)

    @available_if(_classifier_has("decision_function"))
    def decision_function(self, X):
        check_is_fitted(self)
        return self.classifier_.decision_function(X)

# define your clusterer and classifier - we're using KMeans and KNeighbors here
clusterer = KMeans(n_clusters=100, random_state=42) # using k=100, random state just ensures you get the same results each time
classifier = KNeighborsClassifier(n_neighbors=10)

# train the inductive clusterer on the 1949 longitude and latitude data
inductive_learner = InductiveClusterer(clusterer, classifier).fit(spatial_cluster_train[['LONGITUDE', 'LATITUDE']])

# generate cleaned versions of the full dataset with no missing TAVG or PRCP values
# I recommend doing each parameter separately as some stations may have one parameter but not another
cleaned_tavg_df = gsoy_df.dropna(subset=['TAVG']).sort_values('DATE')
cleaned_precip_df = gsoy_df.dropna(subset=['PRCP']).sort_values('DATE')

# save the coordinates of the cluster centers (i.e., the K-means centroids)
cluster_centers = inductive_learner.clusterer_.cluster_centers_

# bin and save the binned data!
binned_data = []
for year in np.arange(1950,2025,1,dtype=int): # start in 1950, end in 2024 (lots of missing stations in 2025)
    
    # from cleaned data, select only data in a given year
    tavg_raw = cleaned_tavg_df[cleaned_tavg_df['DATE']==year][['LONGITUDE', 'LATITUDE', 'TAVG']]
    # tag each station with data for a parameter in a given year with the cluster they belong to
    tavg_raw['Cluster_ID'] = inductive_learner.predict(tavg_raw[['LONGITUDE', 'LATITUDE']])
    
    # same as above for PRCP (and repeat for any other parameter we want to look at)
    prcp_raw = cleaned_precip_df[cleaned_precip_df['DATE']==year][['LONGITUDE', 'LATITUDE', 'PRCP']]
    prcp_raw['Cluster_ID'] = inductive_learner.predict(prcp_raw[['LONGITUDE', 'LATITUDE']])
    
    # calculate mean and median values of each parameter (here TAVG and PRCP) for each cluster
    for cluster_id in range(100):
        mean_tavg = np.mean(tavg_raw[tavg_raw['Cluster_ID']==cluster_id]['TAVG'])
        mean_prcp = np.mean(prcp_raw[prcp_raw['Cluster_ID']==cluster_id]['PRCP'])
        median_tavg = np.median(tavg_raw[tavg_raw['Cluster_ID']==cluster_id]['TAVG'])
        median_prcp = np.median(prcp_raw[prcp_raw['Cluster_ID']==cluster_id]['PRCP'])
        cluster_lon, cluster_lat = cluster_centers[cluster_id]
        binned_data.append([cluster_id, year, cluster_lon, cluster_lat, mean_tavg, median_tavg, mean_prcp, median_prcp])
        
    # for each year, plot the decision boundaries and stations in each cluster (can comment this block out if unneeded)
    # I used the stations with TAVG here since there are fewer of them so the plot would be (slightly) less chaotic
    fig, ax = plt.subplots(figsize=(7.5,5), constrained_layout=True)
    DecisionBoundaryDisplay.from_estimator(inductive_learner, tavg_raw[['LONGITUDE', 'LATITUDE']], response_method="predict", alpha=0.3, 
                                           plot_method='pcolormesh', cmap='tab20', xlabel='Longitude', ylabel='Latitude', ax=ax)
    ax.scatter(tavg_raw['LONGITUDE'].values, tavg_raw['LATITUDE'].values, c=tavg_raw['Cluster_ID'], 
               alpha=0.7, marker='.', edgecolor='none', cmap='tab20')
    ax.set_xlim(-126,-66)
    ax.set_ylim(23.5,50)
    fig.suptitle(year)
    fig.savefig(f'k100_binning_plots/{year}.png')
    plt.close()

# convert binned data to DataFrame and save as CSV
binned_df = pd.DataFrame(binned_data, columns=['Cluster_ID', 'Year', 'Lon', 'Lat', 'TAVG_Avg', 'TAVG_Med', 'PRCP_Avg', 'PRCP_Med'])
binned_df.to_csv('data/binned_k100_1950to2024.csv', index=False)

# read in all the year-by-year cluster maps and make them a GIF (again, you can comment this out)
from PIL import Image
import glob

images = []
for fname in sorted(glob.glob('k100_binning_plots/*.png')): # loop through all png files in the folder
    im = Image.open(fname) # open the image
    images.append(im) # add the image to the list
images[0].save('k100_binning_animated.gif', save_all=True, append_images=images[1:], optimize=False, duration=500, loop=0)